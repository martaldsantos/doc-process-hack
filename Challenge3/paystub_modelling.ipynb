{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 03: Data Modelling: From Retrieval to Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will structure the data retrieved from Azure Document Intelligence (ADI) into the right format to be read by our systems in subsequent steps. \n",
    "\n",
    "The data will be outputted from the ADI as a JSON file, and it is our role to process and organize it. Some of the data will be structured into tables, while other data will be formatted as text. This step ensures that the extracted information is organized in a meaningful way for further analysis and usage.\n",
    "\n",
    "As stated before, we need to make sure that our Function will know how to process:\n",
    "- **Loan Forms:** Extract relevant details such as borrower information, loan amounts, and terms.\n",
    "- **Loan Contract:** Identify and parse key contract elements like clauses, signatures, and dates.\n",
    "- **Pay Stubs:** Retrieve data such as employee details, earnings, deductions, and net pay.\n",
    "\n",
    "Not all customers will have provided all types of content, and during this Challenge we will be only be processing one file. We will combine in the next challenge the capabilities of a trigger, which will, at a time, also process one single document.\n",
    "\n",
    "Due to the nature of this challenge, we will separate this challenge in the 3 different types of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pay Stub "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of some loan applications, the pay stub is a required document. The pay stub is a document that outlines the details of an employee’s income. It contains the employee’s wages earned, applicable deductions and total gross pay, and net pay for the pay period. A pay stub will provide Contoso bank with crucial information about not only a person’s income and employment stability, which helps assess their ability to repay the loan. It also verifies the applicant’s financial credibility and ensures that their reported income matches their actual earnings.\n",
    "\n",
    "When processing a Pay Stub, we will have similar challenges as we previously did on the Loan Forms. These particular documents combine text and contrary to the previous use case, more than 1 table, Once again, the ADI capabilities allows you to extract these 2 types of entities as also separate capabilities.\n",
    "\n",
    "As we've previously create a the function that will load the documents inside a designated folder, all we have to do now is to retrieve all the information inside the paystub folder, we will retrieve one single Loan Form for us to analyse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def read_json_files_from_blob(folder_path):\n",
    "    # Retrieve the connection string from the environment variables\n",
    "    connection_string = os.getenv('STORAGE_CONNECTION_STRING')\n",
    "\n",
    "    # Ensure the connection string is not None\n",
    "    if connection_string is None:\n",
    "        raise ValueError(\"The connection string environment variable is not set.\")\n",
    "\n",
    "    # Create a BlobServiceClient\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "    # Get the container client\n",
    "    container_client = blob_service_client.get_container_client(\"data\")\n",
    "\n",
    "    # List all blobs in the specified folder\n",
    "    blob_list = container_client.list_blobs(name_starts_with=folder_path)\n",
    "\n",
    "    # Filter out JSON files and read their contents\n",
    "    for blob in blob_list:\n",
    "        if blob.name.endswith('.json'):\n",
    "            blob_client = container_client.get_blob_client(blob.name)\n",
    "            blob_data = blob_client.download_blob().readall()\n",
    "            data = json.loads(blob_data)\n",
    "            return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "paystub = read_json_files_from_blob(\"paystubs\") ## RETIRAR PARA ELES PERCEBEREM OQ TAO A FAZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100002\n",
      "JSON file updated successfully.\n"
     ]
    }
   ],
   "source": [
    "def clean_form_recognizer_result(data):\n",
    "    text_content = []\n",
    "    \n",
    "    for page in data.get(\"pages\", []):\n",
    "        for line in page.get(\"lines\", []):\n",
    "            # Check if the line contains the word \"table\"\n",
    "            if \"table\" in line.get(\"text\", \"\").lower():\n",
    "                continue  # Keep everything if \"table\" is in the text\n",
    "            # Keep only the \"text\" key\n",
    "            line_keys = list(line.keys())\n",
    "            for key in line_keys:\n",
    "                if key != \"text\":\n",
    "                    del line[key]\n",
    "            # Collect the text content\n",
    "            text_content.append(line.get(\"text\", \"\"))\n",
    "    \n",
    "    # Create structured tables\n",
    "    structured_tables = create_structured_tables(data.get(\"tables\", []))\n",
    "    \n",
    "    # Concatenate all text content into a single string\n",
    "    plain_text_content = \" \".join(text_content)\n",
    "    \n",
    "    data[\"structured_tables\"] = structured_tables\n",
    "    data[\"plain_text_content\"] = plain_text_content\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_pay_stub(pay_stub_text):\n",
    "        # Dictionary to store parsed data\n",
    "        parsed_data = {}\n",
    "\n",
    "        # Regular expressions to match the required fields\n",
    "        pay_stub_patterns = {\n",
    "            'id': r'Customer ID: (\\d+)',\n",
    "            'Company Name': r'^(.+?) Pay Stub for:',\n",
    "            'Employee Name': r'Pay Stub for: (.+?) Pay Period:',\n",
    "            'Pay Period': r'Pay Period: (.+?) Pay Date:',\n",
    "            'Pay Date': r'Pay Date: (.+?) Employee ID:',\n",
    "            'Employee ': r'Employee ID: (.+?) Employee Information:',\n",
    "            'Employee Address': r'Address: (.+?), Social Security',\n",
    "            'Social_Security': r'Social Security Number: (XXX-XX-\\d{4})'\n",
    "        }\n",
    "\n",
    "        # Apply regex patterns and store matches in the dictionary\n",
    "        for key, pattern in pay_stub_patterns.items():\n",
    "            match = re.search(pattern, pay_stub_text)\n",
    "            if match:\n",
    "                parsed_data[key] = match.group(1)\n",
    "        return parsed_data\n",
    "\n",
    "def create_structured_tables(tables):\n",
    "    structured_tables = []\n",
    "    for table in tables:\n",
    "        row_count = table.get(\"row_count\", 0)\n",
    "        column_count = table.get(\"column_count\", 0)\n",
    "        cells = table.get(\"cells\", [])\n",
    "        \n",
    "        # Initialize an empty table\n",
    "        structured_table = [[\"\" for _ in range(column_count)] for _ in range(row_count)]\n",
    "        \n",
    "        # Populate the table with cell content\n",
    "        for cell in cells:\n",
    "            row_index = cell.get(\"row_index\", 0)\n",
    "            column_index = cell.get(\"column_index\", 0)\n",
    "            content = cell.get(\"content\", \"\")\n",
    "            structured_table[row_index][column_index] = content\n",
    "        \n",
    "        structured_tables.append(structured_table)\n",
    "    \n",
    "    return structured_tables\n",
    "\n",
    "def tables_to_dataframes(structured_tables):\n",
    "    dataframes = []\n",
    "    for table in structured_tables:\n",
    "        df = pd.DataFrame(table)\n",
    "        dataframes.append(df)\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "\n",
    "cleaned_data = clean_form_recognizer_result(paystub)\n",
    "dataframes = tables_to_dataframes(cleaned_data[\"structured_tables\"])\n",
    "\n",
    "structured_data = {\n",
    "    \"pay stub details\": parse_pay_stub(cleaned_data[\"plain_text_content\"]),\n",
    "}\n",
    "\n",
    "\n",
    "df_list = []\n",
    "\n",
    "def process_dataframe(df):\n",
    "    result = {}\n",
    "    columns = df.columns[1:]  # Ignore the first column\n",
    "    for i in range(1, len(df)):  # Ignore the first row\n",
    "        row_name = df.iloc[i, 0]\n",
    "        result[row_name] = {}\n",
    "        for col in columns:\n",
    "            result[row_name][col] = f\"{row_name} {col}: {df.at[i, col]}\"\n",
    "    return result\n",
    "\n",
    "def rename_json_attributes(json_obj, attribute_titles):\n",
    "    \"\"\"\n",
    "    Rename the keys of a JSON object based on the provided attribute titles.\n",
    "\n",
    "    Parameters:\n",
    "    json_obj (dict): The JSON object to rename.\n",
    "    attribute_titles (dict): A dictionary where keys are the current attribute names and values are the new attribute names.\n",
    "\n",
    "    Returns:\n",
    "    dict: The updated JSON object with renamed keys.\n",
    "    \"\"\"\n",
    "    updated_json = {}\n",
    "    for old_key, new_key in attribute_titles.items():\n",
    "        if old_key in json_obj:\n",
    "            updated_json[new_key] = json_obj[old_key]\n",
    "        else:\n",
    "            updated_json[old_key] = json_obj.get(old_key, None)\n",
    "    return updated_json\n",
    "\n",
    "attribute_titles_earnings = {\n",
    "    \"1\": \"Hours Worked\",\n",
    "    \"2\": \"Rate\",\n",
    "    \"3\": \"Current Earnings\",\n",
    "    \"4\": \"Year-to-Date Earnings\"\n",
    "}\n",
    "\n",
    "attribute_titles_deductions = {\n",
    "    \"1\": \"Current Amount\",\n",
    "    \"2\": \"Year-to-Date Amount\"\n",
    "}\n",
    "\n",
    "# Process the earnings and deductions DataFrames\n",
    "earnings_dict = process_dataframe(dataframes[0])\n",
    "deductions_dict = process_dataframe(dataframes[1])\n",
    "# Append the processed DataFrames to the JSON structure\n",
    "structured_data[\"earnings\"] = earnings_dict\n",
    "structured_data[\"deductions\"] = deductions_dict\n",
    "\n",
    "def clean_pay_stub_section(data):\n",
    "    # Check for 'deductions' and 'earnings' in the data\n",
    "    for section in ['deductions', 'earnings']:\n",
    "        if section in data:\n",
    "            for key, values in data[section].items():\n",
    "                # For each entry, clean up the values by removing everything before the colon\n",
    "                for subkey in values:\n",
    "                    # Split the string by colon and take the second part, stripping whitespace\n",
    "                    values[subkey] = values[subkey].split(\":\")[1].strip()\n",
    "    return data\n",
    "\n",
    "structured_data = clean_pay_stub_section(structured_data)\n",
    "\n",
    "\n",
    "def update_attribute_keys(data, section, key_mapping):\n",
    "    # Ensure the section exists in the data (either \"earnings\" or \"deductions\")\n",
    "    if section in data:\n",
    "        # Iterate over each type within the earnings or deductions section\n",
    "        for entry_type, attributes in data[section].items():\n",
    "            # Create a new dictionary to store the updated attributes\n",
    "            updated_attributes = {}\n",
    "            \n",
    "            # Loop through each attribute in that entry (e.g. 1, 2, 3)\n",
    "            for old_key, value in attributes.items():\n",
    "                # Map the old key (which is an integer) to the new descriptive key using key_mapping\n",
    "                if str(old_key) in key_mapping:  # Convert old_key to string to match the mapping\n",
    "                    new_key = key_mapping[str(old_key)]\n",
    "                else:\n",
    "                    new_key = old_key  # If no mapping is found, retain the old key\n",
    "                \n",
    "                # Update the dictionary with the new key\n",
    "                updated_attributes[new_key] = value\n",
    "\n",
    "            # Replace the old attributes with the updated attributes in the data\n",
    "            data[section][entry_type] = updated_attributes\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "paystub_final = update_attribute_keys(structured_data, \"earnings\", attribute_titles_earnings)\n",
    "paystub_final = structured_data = update_attribute_keys(structured_data, \"deductions\", attribute_titles_deductions)\n",
    "\n",
    "customer_id = paystub_final[\"pay stub details\"][\"id\"]\n",
    "\n",
    "print(customer_id)\n",
    "# Save the updated JSON structure back to the file\n",
    "json_data = json.dumps(paystub_final, indent=4)\n",
    "\n",
    "\n",
    "print(\"JSON file updated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cosmos import CosmosClient, exceptions, PartitionKey\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Cosmos DB connection details from environment variables\n",
    "endpoint = os.getenv(\"COSMOS_ENDPOINT\")\n",
    "key = os.getenv(\"COSMOS_KEY\")\n",
    "\n",
    "def upload_text_to_cosmos_db(text_content, container_name):\n",
    "    # Check if the text is empty\n",
    "    if not text_content:\n",
    "        print(\"The text content is empty. No data to upload.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize the Cosmos client\n",
    "    client = CosmosClient(endpoint, key)\n",
    "    \n",
    "    try:\n",
    "        # Create or get the database\n",
    "        database = client.create_database_if_not_exists(id=\"ContosoDB\")\n",
    "        \n",
    "        # Create or get the container\n",
    "        container = database.create_container_if_not_exists(\n",
    "            id=container_name,\n",
    "            partition_key=PartitionKey(path=f\"/id\"),\n",
    "            offer_throughput=400\n",
    "        )\n",
    "    except exceptions.CosmosHttpResponseError as e:\n",
    "        print(f\"An error occurred while creating the database or container: {e.message}\")\n",
    "        return\n",
    "    \n",
    "    # Create a document with the text content and partition key\n",
    "    document = {\n",
    "        'id': str(customer_id),  # Generate a unique ID for the document\n",
    "        'content': text_content,  # Store the plain text as 'content'\n",
    "    }\n",
    "    \n",
    "    # Upload the document to the container\n",
    "    try:\n",
    "        container.create_item(body=document)\n",
    "        print(f\"Text content uploaded successfully with ID '{document['id']}' in Cosmos DB.\")\n",
    "    except exceptions.CosmosHttpResponseError as e:\n",
    "        print(f\"An error occurred while uploading the document: {e.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Pay Stubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while uploading the document: (Conflict) Entity with the specified id already exists in the system., \n",
      "RequestStartTime: 2024-09-30T12:06:17.8759615Z, RequestEndTime: 2024-09-30T12:06:17.8774628Z,  Number of regions attempted:1\n",
      "{\"systemHistory\":[{\"dateUtc\":\"2024-09-30T12:05:19.3977211Z\",\"cpu\":0.383,\"memory\":659431532.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0978,\"availableThreads\":32765,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":428},{\"dateUtc\":\"2024-09-30T12:05:29.4077347Z\",\"cpu\":0.639,\"memory\":659426500.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0605,\"availableThreads\":32765,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":428},{\"dateUtc\":\"2024-09-30T12:05:39.4179698Z\",\"cpu\":0.400,\"memory\":659450816.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0376,\"availableThreads\":32763,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":428},{\"dateUtc\":\"2024-09-30T12:05:49.4275521Z\",\"cpu\":0.246,\"memory\":659448272.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.031,\"availableThreads\":32765,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":428},{\"dateUtc\":\"2024-09-30T12:05:59.4383168Z\",\"cpu\":0.139,\"memory\":659446832.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0529,\"availableThreads\":32765,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":428},{\"dateUtc\":\"2024-09-30T12:06:09.4482371Z\",\"cpu\":0.429,\"memory\":659448260.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0718,\"availableThreads\":32765,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":427}]}\n",
      "RequestStart: 2024-09-30T12:06:17.8760403Z; ResponseTime: 2024-09-30T12:06:17.8774628Z; StoreResult: StorePhysicalAddress: rntbd://10.0.2.40:14300/apps/c80aa009-dda2-405e-8914-473bbd79be7d/services/af267358-981c-4a1c-8e76-a40a7dd8e20a/partitions/db9a23cc-b9a6-415a-827a-e1c2bdbb8b53/replicas/133721589631137497p, LSN: 4, GlobalCommittedLsn: 4, PartitionKeyRangeId: 0, IsValid: True, StatusCode: 409, SubStatusCode: 0, RequestCharge: 5.1, ItemLSN: -1, SessionToken: -1#4, UsingLocalLSN: False, TransportException: null, BELatencyMs: 0.889, ActivityId: f56b0a4c-58ba-4f09-b06d-d7481dcc6d34, RetryAfterInMs: , ReplicaHealthStatuses: [(port: 14300 | status: Connected | lkt: 9/30/2024 12:05:47 PM)], TransportRequestTimeline: {\"requestTimeline\":[{\"event\": \"Created\", \"startTimeUtc\": \"2024-09-30T12:06:17.8760410Z\", \"durationInMs\": 0.0132},{\"event\": \"ChannelAcquisitionStarted\", \"startTimeUtc\": \"2024-09-30T12:06:17.8760542Z\", \"durationInMs\": 0.009},{\"event\": \"Pipelined\", \"startTimeUtc\": \"2024-09-30T12:06:17.8760632Z\", \"durationInMs\": 0.0676},{\"event\": \"Transit Time\", \"startTimeUtc\": \"2024-09-30T12:06:17.8761308Z\", \"durationInMs\": 1.246},{\"event\": \"Received\", \"startTimeUtc\": \"2024-09-30T12:06:17.8773768Z\", \"durationInMs\": 0.0724},{\"event\": \"Completed\", \"startTimeUtc\": \"2024-09-30T12:06:17.8774492Z\", \"durationInMs\": 0}],\"serviceEndpointStats\":{\"inflightRequests\":1,\"openConnections\":1},\"connectionStats\":{\"waitforConnectionInit\":\"False\",\"callsPendingReceive\":0,\"lastSendAttempt\":\"2024-09-30T12:06:07.3289122Z\",\"lastSend\":\"2024-09-30T12:06:07.3289494Z\",\"lastReceive\":\"2024-09-30T12:06:07.3294968Z\"},\"requestSizeInBytes\":1817,\"requestBodySizeInBytes\":1297,\"responseMetadataSizeInBytes\":182,\"responseBodySizeInBytes\":65};\n",
      " ResourceType: Document, OperationType: Create\n",
      ", Microsoft.Azure.Documents.Common/2.14.0\n",
      "Code: Conflict\n",
      "Message: Entity with the specified id already exists in the system., \n",
      "RequestStartTime: 2024-09-30T12:06:17.8759615Z, RequestEndTime: 2024-09-30T12:06:17.8774628Z,  Number of regions attempted:1\n",
      "{\"systemHistory\":[{\"dateUtc\":\"2024-09-30T12:05:19.3977211Z\",\"cpu\":0.383,\"memory\":659431532.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0978,\"availableThreads\":32765,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":428},{\"dateUtc\":\"2024-09-30T12:05:29.4077347Z\",\"cpu\":0.639,\"memory\":659426500.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0605,\"availableThreads\":32765,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":428},{\"dateUtc\":\"2024-09-30T12:05:39.4179698Z\",\"cpu\":0.400,\"memory\":659450816.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0376,\"availableThreads\":32763,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":428},{\"dateUtc\":\"2024-09-30T12:05:49.4275521Z\",\"cpu\":0.246,\"memory\":659448272.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.031,\"availableThreads\":32765,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":428},{\"dateUtc\":\"2024-09-30T12:05:59.4383168Z\",\"cpu\":0.139,\"memory\":659446832.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0529,\"availableThreads\":32765,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":428},{\"dateUtc\":\"2024-09-30T12:06:09.4482371Z\",\"cpu\":0.429,\"memory\":659448260.000,\"threadInfo\":{\"isThreadStarving\":\"False\",\"threadWaitIntervalInMs\":0.0718,\"availableThreads\":32765,\"minThreads\":64,\"maxThreads\":32767},\"numberOfOpenTcpConnection\":427}]}\n",
      "RequestStart: 2024-09-30T12:06:17.8760403Z; ResponseTime: 2024-09-30T12:06:17.8774628Z; StoreResult: StorePhysicalAddress: rntbd://10.0.2.40:14300/apps/c80aa009-dda2-405e-8914-473bbd79be7d/services/af267358-981c-4a1c-8e76-a40a7dd8e20a/partitions/db9a23cc-b9a6-415a-827a-e1c2bdbb8b53/replicas/133721589631137497p, LSN: 4, GlobalCommittedLsn: 4, PartitionKeyRangeId: 0, IsValid: True, StatusCode: 409, SubStatusCode: 0, RequestCharge: 5.1, ItemLSN: -1, SessionToken: -1#4, UsingLocalLSN: False, TransportException: null, BELatencyMs: 0.889, ActivityId: f56b0a4c-58ba-4f09-b06d-d7481dcc6d34, RetryAfterInMs: , ReplicaHealthStatuses: [(port: 14300 | status: Connected | lkt: 9/30/2024 12:05:47 PM)], TransportRequestTimeline: {\"requestTimeline\":[{\"event\": \"Created\", \"startTimeUtc\": \"2024-09-30T12:06:17.8760410Z\", \"durationInMs\": 0.0132},{\"event\": \"ChannelAcquisitionStarted\", \"startTimeUtc\": \"2024-09-30T12:06:17.8760542Z\", \"durationInMs\": 0.009},{\"event\": \"Pipelined\", \"startTimeUtc\": \"2024-09-30T12:06:17.8760632Z\", \"durationInMs\": 0.0676},{\"event\": \"Transit Time\", \"startTimeUtc\": \"2024-09-30T12:06:17.8761308Z\", \"durationInMs\": 1.246},{\"event\": \"Received\", \"startTimeUtc\": \"2024-09-30T12:06:17.8773768Z\", \"durationInMs\": 0.0724},{\"event\": \"Completed\", \"startTimeUtc\": \"2024-09-30T12:06:17.8774492Z\", \"durationInMs\": 0}],\"serviceEndpointStats\":{\"inflightRequests\":1,\"openConnections\":1},\"connectionStats\":{\"waitforConnectionInit\":\"False\",\"callsPendingReceive\":0,\"lastSendAttempt\":\"2024-09-30T12:06:07.3289122Z\",\"lastSend\":\"2024-09-30T12:06:07.3289494Z\",\"lastReceive\":\"2024-09-30T12:06:07.3294968Z\"},\"requestSizeInBytes\":1817,\"requestBodySizeInBytes\":1297,\"responseMetadataSizeInBytes\":182,\"responseBodySizeInBytes\":65};\n",
      " ResourceType: Document, OperationType: Create\n",
      ", Microsoft.Azure.Documents.Common/2.14.0\n"
     ]
    }
   ],
   "source": [
    "upload_text_to_cosmos_db(paystub_final, \"PayStubs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
